{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6db1da5-bb4a-4c93-bf03-482e588854f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 785)\n",
      "(10000, 785)\n"
     ]
    }
   ],
   "source": [
    "import pandas  as pd\n",
    "train_df = pd.read_csv('fashion-mnist_train.csv')\n",
    "test_df = pd.read_csv('fashion-mnist_test.csv')\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f807c4e-041a-4ddd-ab19-eadb9ad0bfa5",
   "metadata": {},
   "source": [
    "loading the dataset\n",
    "first, i imported the pandas library , then, loadeded the fashion-mnist training and test datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10f76034-a832-4859-8457-de0f6877832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class  FashionMNISTCSV(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.labels = dataframe.iloc[:, 0].values.astype(np.int64)\n",
    "        self.images = dataframe.iloc[:, 1:].values.astype(np.uint8)\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx].reshape(28, 28) \n",
    "        image = Image.fromarray(image)  \n",
    "        image = image.convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = self.labels[idx]\n",
    "        return image, label\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),                    \n",
    "    transforms.ToTensor(),                            \n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) \n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = FashionMNISTCSV(train_df, transform=transform)\n",
    "test_dataset = FashionMNISTCSV(test_df, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16eef179-6f64-4a0f-a75f-95a004a6673e",
   "metadata": {},
   "source": [
    "creating a custom dataset and dataloader\n",
    "defined a custom dataset class to load images and labels from the csv dataframes , the images are reshaped to 28x28 and converted to rgb since pretrained models expect 3 channels ,also applied transforms to resize images to 224x224, convert them to tensors, and normalize them .\n",
    "also created dataset objects for training and testing, and wrapped them in dataloaders to load data in batches of 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f17754b-0b75-45bf-8ef5-76620c65df54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=2048, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "model = models.resnet50(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 10)\n",
    "print(model.fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7bb0fe-7e39-4f56-bdb8-8db2cc15ff23",
   "metadata": {},
   "source": [
    "loading pretrained resnet50 and modifying final layer\n",
    "i loaded the pretrained resnet50 model and froze all its parameters so they won’t update during training , then, i replaced the final fully connected layer to output 10 classes, matching the number of categories in the fashion mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e73aec2-fe59-48a5-82a9-7929938f569a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=2048, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import torch.nn as nn\n",
    "weights = ResNet50_Weights.DEFAULT\n",
    "model = resnet50(weights=weights)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 10)\n",
    "print(model.fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d9fd50-33cb-466b-9a89-ec92b19806ad",
   "metadata": {},
   "source": [
    "loading resnet50 with updated weights syntax\n",
    "i loaded the resnet50 model using the recommended weights syntax , all layers were frozen to prevent training them , then i replaced the final fully connected layer to output 10 classes for fashion mnist,then, i printed the modified layer to confirm the update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "639efb29-0d0b-4d75-a171-da1778d69566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "device = torch.device('cuda')\n",
    "print(f\"Using device: {device}\")\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8c7c3f-aadc-44e8-b97b-768223ab570c",
   "metadata": {},
   "source": [
    "setting device, loss function, and optimizer\n",
    "i set the device to use gpu , then i moved the model to that device , i used cross entropy loss since it's a multi-class classification task and chose the adam optimizer to update only the final layer's parameters with a learning rate of 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88d9ca62-45b0-4bf2-a8db-a8db90e4d1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 2.3451\n",
      "Batch 100, Loss: 1.0641\n",
      "Batch 200, Loss: 0.6754\n",
      "Batch 300, Loss: 0.6531\n",
      "Batch 400, Loss: 0.5825\n",
      "Batch 500, Loss: 0.5588\n",
      "Batch 600, Loss: 0.4380\n",
      "Batch 700, Loss: 0.6069\n",
      "Batch 800, Loss: 0.4584\n",
      "Batch 900, Loss: 0.4279\n"
     ]
    }
   ],
   "source": [
    "model.train()  \n",
    "for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    optimizer.zero_grad()     \n",
    "    outputs = model(images)   \n",
    "    loss = criterion(outputs, labels) \n",
    "    loss.backward()\n",
    "    optimizer.step()            \n",
    "    if batch_idx % 100 == 0:   \n",
    "        print(f'Batch {batch_idx}, Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9c10bf-edc1-489e-84c2-874312fe7968",
   "metadata": {},
   "source": [
    "training loop for one epoch\n",
    "i set the model to training mode and looped over the training data , each batch was moved to the selected device, loss was calculated, and backpropagation was done to update the model’s final layer , i printed the loss every 100 batches to monitor training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "155eb31a-4127-4a02-9587-2edcf2ce406a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 83.46%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Validation Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff81e00-618d-4eee-a6dc-4e2f515e1fe1",
   "metadata": {},
   "source": [
    "evaluating model on test data\n",
    "i set the model to evaluation mode and disabled gradient calculation , then i looped through the test data, made predictions, and compared them with the true labels to count correct predictions. finally, i calculated and printed the overall validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c96caecc-bc9d-4df8-9320-125e7992313a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Batch [100], Loss: 0.4920\n",
      "Epoch [1/5], Batch [200], Loss: 0.4823\n",
      "Epoch [1/5], Batch [300], Loss: 0.4914\n",
      "Epoch [1/5], Batch [400], Loss: 0.4764\n",
      "Epoch [1/5], Batch [500], Loss: 0.4739\n",
      "Epoch [1/5], Batch [600], Loss: 0.4790\n",
      "Epoch [1/5], Batch [700], Loss: 0.4491\n",
      "Epoch [1/5], Batch [800], Loss: 0.4611\n",
      "Epoch [1/5], Batch [900], Loss: 0.4570\n",
      "Validation Accuracy after epoch 1: 84.89%\n",
      "Epoch [2/5], Batch [100], Loss: 0.4250\n",
      "Epoch [2/5], Batch [200], Loss: 0.4353\n",
      "Epoch [2/5], Batch [300], Loss: 0.4470\n",
      "Epoch [2/5], Batch [400], Loss: 0.4265\n",
      "Epoch [2/5], Batch [500], Loss: 0.4250\n",
      "Epoch [2/5], Batch [600], Loss: 0.4210\n",
      "Epoch [2/5], Batch [700], Loss: 0.4120\n",
      "Epoch [2/5], Batch [800], Loss: 0.4177\n",
      "Epoch [2/5], Batch [900], Loss: 0.4098\n",
      "Validation Accuracy after epoch 2: 85.05%\n",
      "Epoch [3/5], Batch [100], Loss: 0.4024\n",
      "Epoch [3/5], Batch [200], Loss: 0.4026\n",
      "Epoch [3/5], Batch [300], Loss: 0.3938\n",
      "Epoch [3/5], Batch [400], Loss: 0.3943\n",
      "Epoch [3/5], Batch [500], Loss: 0.4100\n",
      "Epoch [3/5], Batch [600], Loss: 0.3924\n",
      "Epoch [3/5], Batch [700], Loss: 0.4054\n",
      "Epoch [3/5], Batch [800], Loss: 0.3952\n",
      "Epoch [3/5], Batch [900], Loss: 0.3834\n",
      "Validation Accuracy after epoch 3: 86.18%\n",
      "Epoch [4/5], Batch [100], Loss: 0.3680\n",
      "Epoch [4/5], Batch [200], Loss: 0.3877\n",
      "Epoch [4/5], Batch [300], Loss: 0.3840\n",
      "Epoch [4/5], Batch [400], Loss: 0.3758\n",
      "Epoch [4/5], Batch [500], Loss: 0.3957\n",
      "Epoch [4/5], Batch [600], Loss: 0.3815\n",
      "Epoch [4/5], Batch [700], Loss: 0.3903\n",
      "Epoch [4/5], Batch [800], Loss: 0.3755\n",
      "Epoch [4/5], Batch [900], Loss: 0.3813\n",
      "Validation Accuracy after epoch 4: 86.28%\n",
      "Epoch [5/5], Batch [100], Loss: 0.3744\n",
      "Epoch [5/5], Batch [200], Loss: 0.3692\n",
      "Epoch [5/5], Batch [300], Loss: 0.3700\n",
      "Epoch [5/5], Batch [400], Loss: 0.3676\n",
      "Epoch [5/5], Batch [500], Loss: 0.3684\n",
      "Epoch [5/5], Batch [600], Loss: 0.3616\n",
      "Epoch [5/5], Batch [700], Loss: 0.3678\n",
      "Epoch [5/5], Batch [800], Loss: 0.3633\n",
      "Epoch [5/5], Batch [900], Loss: 0.3754\n",
      "Validation Accuracy after epoch 5: 85.60%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5 \n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 100 == 99:  \n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}], Loss: {running_loss/100:.4f}')\n",
    "            running_loss = 0.0\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Validation Accuracy after epoch {epoch+1}: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3805c202-9939-458d-b616-4d1ea86c7f76",
   "metadata": {},
   "source": [
    "training and validating for multiple epochs\n",
    "i trained the model for 5 epochs , in each epoch, i ran the training loop and printed the average loss every 100 batches , after each epoch, i evaluated the model on the test set and printed the validation accuracy to track progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9fb1f93-3ae0-478a-b7e2-9ac3456ea2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a49d29f-9672-4314-92fa-7892b2c3ecab",
   "metadata": {},
   "source": [
    "fine-tuning last resnet block\n",
    "i unfroze the parameters of layer4 to fine-tune the last block of resnet50 , then, i created a new optimizer that only updates the trainable parameters and added a learning rate scheduler to gradually reduce the learning rate after every epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f4f4f83-7f3d-4589-a4fb-be2084d6c0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tune Epoch [1/3], Batch [0], Loss: 0.5315\n",
      "Fine-Tune Epoch [1/3], Batch [100], Loss: 0.4464\n",
      "Fine-Tune Epoch [1/3], Batch [200], Loss: 0.3593\n",
      "Fine-Tune Epoch [1/3], Batch [300], Loss: 0.3219\n",
      "Fine-Tune Epoch [1/3], Batch [400], Loss: 0.2225\n",
      "Fine-Tune Epoch [1/3], Batch [500], Loss: 0.3468\n",
      "Fine-Tune Epoch [1/3], Batch [600], Loss: 0.4317\n",
      "Fine-Tune Epoch [1/3], Batch [700], Loss: 0.1693\n",
      "Fine-Tune Epoch [1/3], Batch [800], Loss: 0.4337\n",
      "Fine-Tune Epoch [1/3], Batch [900], Loss: 0.2862\n",
      "Validation Accuracy after fine-tune epoch 1: 88.38%\n",
      "\n",
      "Fine-Tune Epoch [2/3], Batch [0], Loss: 0.1925\n",
      "Fine-Tune Epoch [2/3], Batch [100], Loss: 0.1518\n",
      "Fine-Tune Epoch [2/3], Batch [200], Loss: 0.2908\n",
      "Fine-Tune Epoch [2/3], Batch [300], Loss: 0.2800\n",
      "Fine-Tune Epoch [2/3], Batch [400], Loss: 0.3049\n",
      "Fine-Tune Epoch [2/3], Batch [500], Loss: 0.2674\n",
      "Fine-Tune Epoch [2/3], Batch [600], Loss: 0.2793\n",
      "Fine-Tune Epoch [2/3], Batch [700], Loss: 0.2903\n",
      "Fine-Tune Epoch [2/3], Batch [800], Loss: 0.2903\n",
      "Fine-Tune Epoch [2/3], Batch [900], Loss: 0.1737\n",
      "Validation Accuracy after fine-tune epoch 2: 89.39%\n",
      "\n",
      "Fine-Tune Epoch [3/3], Batch [0], Loss: 0.2403\n",
      "Fine-Tune Epoch [3/3], Batch [100], Loss: 0.1731\n",
      "Fine-Tune Epoch [3/3], Batch [200], Loss: 0.2677\n",
      "Fine-Tune Epoch [3/3], Batch [300], Loss: 0.2580\n",
      "Fine-Tune Epoch [3/3], Batch [400], Loss: 0.4878\n",
      "Fine-Tune Epoch [3/3], Batch [500], Loss: 0.1705\n",
      "Fine-Tune Epoch [3/3], Batch [600], Loss: 0.1628\n",
      "Fine-Tune Epoch [3/3], Batch [700], Loss: 0.1954\n",
      "Fine-Tune Epoch [3/3], Batch [800], Loss: 0.1142\n",
      "Fine-Tune Epoch [3/3], Batch [900], Loss: 0.1053\n",
      "Validation Accuracy after fine-tune epoch 3: 89.67%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_finetune_epochs = 3  \n",
    "for epoch in range(num_finetune_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Fine-Tune Epoch [{epoch+1}/{num_finetune_epochs}], Batch [{batch_idx}], Loss: {loss.item():.4f}\")\n",
    "    scheduler.step()\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Validation Accuracy after fine-tune epoch {epoch+1}: {accuracy:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16fd728-95b2-4cad-951a-c5d60c0f4421",
   "metadata": {},
   "source": [
    "fine-tuning and validating model\n",
    "i fine-tuned the model for 3 more epochs with layer 4 unfrozen , after training on each epoch, i used the scheduler to adjust the learning rate , then, i evaluated the model on the test set and printed the validation accuracy to track improvement"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
